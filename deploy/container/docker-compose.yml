# Default compose file for x86_64 with Caddy reverse proxy
# Works with both Docker and Podman:
#   Docker (macOS dev):  docker compose up
#   Podman (Linux prod): podman-compose up
#
# Architecture:
#   - Caddy: Reverse proxy, single entry point for all containers
#   - EAVS: LLM proxy, manages virtual keys and usage tracking
#   - Backend: Orchestrates containers, creates EAVS keys per session
#   - Containers: Run opencode with EAVS virtual key for LLM access

networks:
  # Public network - Caddy connects here and exposes ports
  oqto-public:
    driver: bridge
  # Internal network - containers communicate without host exposure
  oqto-internal:
    driver: bridge
    internal: true

services:
  # EAVS - LLM proxy for virtual keys and usage tracking
  # All containers route LLM traffic through EAVS using virtual keys
  # Backend creates/revokes keys via admin API
  eavs:
    image: ghcr.io/eavs/eavs:latest  # TODO: Replace with actual image
    container_name: oqto-eavs
    restart: unless-stopped
    ports:
      - "41800:3000"  # EAVS API (accessible from host and containers via host.docker.internal)
    environment:
      # Master key for admin operations (create/revoke virtual keys)
      - EAVS_MASTER_KEY=${EAVS_MASTER_KEY:-dev-master-key-change-in-production}
      # Upstream LLM provider keys
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Optional: Other providers
      # - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      # - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
    volumes:
      - eavs_data:/data
    networks:
      - oqto-public
      - oqto-internal
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Caddy reverse proxy - single entry point for all containers
  caddy:
    image: caddy:2-alpine
    container_name: oqto-caddy
    restart: unless-stopped
    ports:
      - "41800:80"    # HTTP
      - "41801:443"   # HTTPS
      - "41802:2019"  # Admin API for dynamic route management
    volumes:
      - ./caddy/caddy.json:/etc/caddy/caddy.json:ro
      - caddy_data:/data
      - caddy_config:/config
    command: ["caddy", "run", "--config", "/etc/caddy/caddy.json"]
    networks:
      - oqto-public
      - oqto-internal
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # mmry Embeddings Service - Central hub for embeddings/reranking
  # Per-container mmry instances delegate heavy operations here
  mmry-embeddings:
    image: ghcr.io/byteowlz/mmry:latest  # TODO: Build and publish mmry image
    container_name: oqto-mmry-embeddings
    restart: unless-stopped
    ports:
      - "8081:8081"  # mmry external API
    environment:
      - MMRY_API_KEY=${MMRY_API_KEY:-dev-mmry-key-change-in-production}
    volumes:
      - mmry_data:/var/lib/mmry
      - ./mmry-embeddings.config.toml:/etc/mmry/config.toml:ro
    networks:
      - oqto-public
      - oqto-internal
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8081/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Template container - use this as a reference for spawning new containers
  # In production, the Rust backend will spawn these dynamically
  oqto-template:
    build:
      context: ..
      dockerfile: container/Dockerfile
      args:
        USERNAME: dev
        USER_UID: 1000
        USER_GID: 1000
    image: oqto-dev:x86_64
    # Container is not started by default - it's a template
    profiles:
      - template
    networks:
      - oqto-internal
    volumes:
      - ./workspace:/home/dev/workspace
    environment:
      # Connect to host mmry service for embeddings
      - MMRY_HOST_URL=http://mmry-embeddings:8081
      - MMRY_HOST_KEY=${MMRY_API_KEY:-dev-mmry-key-change-in-production}
    stdin_open: true
    tty: true

volumes:
  caddy_data:
  caddy_config:
  eavs_data:
  mmry_data:
